import {
  isGatewayEnabled,
  buildGeminiApiUrl,
  buildGeminiHeaders,
  buildOpenAIHeaders,
  getOpenAIBaseUrl,
  getDeepSeekBaseUrl,
  buildDeepSeekHeaders,
  buildFalApiUrl,
  buildFalHeaders,
} from "@/lib/cloudflare/ai-gateway";
import { OpenRouterClient } from "@/lib/openrouter/client";
import type {
  AIClientConfig,
  AICompletionOptions,
  AICompletionResponse,
  AIMessage,
} from "@/types/agents";

export interface ModelCapability {
  jsonMode: boolean;
  purpose: "text-generation" | "reasoning" | "image-generation" | "research";
}

export const MODEL_CAPABILITIES: Record<string, ModelCapability> = {
  "deepseek-reasoner": { jsonMode: false, purpose: "reasoning" },
  "deepseek-chat": { jsonMode: true, purpose: "text-generation" },
  "fal-ai/bytedance/seedream/v4/text-to-image": {
    jsonMode: false,
    purpose: "image-generation",
  },
  "fal-ai/qwen-image": { jsonMode: false, purpose: "image-generation" },
  "gemini-3-pro-image-preview": {
    jsonMode: false,
    purpose: "image-generation",
  },
  "gemini-2.5-flash-image": { jsonMode: false, purpose: "image-generation" },
  "gpt-image-1-mini": { jsonMode: false, purpose: "image-generation" },
  "gpt-5-mini": { jsonMode: true, purpose: "text-generation" },
  "perplexity-research": { jsonMode: false, purpose: "research" },
};

export function supportsJsonMode(model: string): boolean {
  const capability = MODEL_CAPABILITIES[model];
  return capability?.jsonMode ?? false;
}

interface DeepSeekMessage {
  role: string;
  content?: string;
  reasoning_content?: string;
  reasoning?: string;
  thinking?: string;
}

interface DeepSeekResponse {
  choices: Array<{
    message: DeepSeekMessage;
    finish_reason?: "stop" | "length" | "content_filter" | null;
  }>;
  usage?: {
    prompt_tokens?: number;
    completion_tokens?: number;
    total_tokens?: number;
  };
  model: string;
}

export class AIClient {
  private config: AIClientConfig;

  constructor(config: AIClientConfig) {
    this.config = config;
  }

  /**
   * 5 å±¤ Fallback æ©Ÿåˆ¶ï¼š
   * 1. Gateway DeepSeek (ç•¶å‰æ¨¡å‹)
   * 2. Gateway OpenRouter DeepSeek (deepseek/deepseek-v3.2)
   * 3. ç›´é€£ DeepSeek API
   * 4. Gateway OpenAI (gpt-5/gpt-5-mini)
   * 5. ç›´é€£ OpenAI
   */
  private async callDeepSeekAPI(params: {
    model: string;
    messages: Array<{ role: string; content: string }>;
    temperature?: number;
    max_tokens?: number;
    response_format?: { type: string };
  }) {
    const deepseekApiKey = process.env.DEEPSEEK_API_KEY;
    const errors: string[] = [];

    // === Step 1: Gateway DeepSeek ===
    if (isGatewayEnabled() && deepseekApiKey) {
      try {
        console.log(`[AIClient] ğŸ”„ Step 1: Gateway DeepSeek (${params.model})`);
        return await this.callDeepSeekAPIInternal(params, deepseekApiKey, true);
      } catch (error: unknown) {
        const err = error instanceof Error ? error : new Error(String(error));
        errors.push(`Gateway DeepSeek: ${err.message}`);
        console.log(`[AIClient] âš ï¸ Step 1 å¤±æ•—: ${err.message}`);
      }
    }

    // === Step 2: Gateway OpenRouter DeepSeek ===
    const openRouterClient = new OpenRouterClient();
    if (openRouterClient.isConfigured()) {
      try {
        console.log(
          "[AIClient] ğŸ”„ Step 2: Gateway OpenRouter (deepseek/deepseek-v3.2)",
        );
        const response = await openRouterClient.chat({
          model: "deepseek/deepseek-v3.2",
          messages: params.messages.map((m) => ({
            role: m.role as "system" | "user" | "assistant",
            content: m.content,
          })),
          temperature: params.temperature,
          max_tokens: params.max_tokens,
          response_format: params.response_format as
            | { type: "text" | "json_object" }
            | undefined,
        });
        console.log(
          "[AIClient] âœ… Step 2 æˆåŠŸ: Gateway OpenRouter (deepseek/deepseek-v3.2)",
        );
        return {
          choices: response.choices.map((c) => ({
            message: { role: c.message.role, content: c.message.content },
          })),
          usage: response.usage,
          model: response.model,
        } as DeepSeekResponse;
      } catch (error: unknown) {
        const err = error instanceof Error ? error : new Error(String(error));
        errors.push(`OpenRouter DeepSeek: ${err.message}`);
        console.log(`[AIClient] âš ï¸ Step 2 å¤±æ•—: ${err.message}`);
      }
    }

    // === Step 3: ç›´é€£ DeepSeek API ===
    if (deepseekApiKey) {
      try {
        console.log(
          `[AIClient] ğŸ”„ Step 3: ç›´é€£ DeepSeek API (${params.model})`,
        );
        const result = await this.callDeepSeekAPIInternal(
          params,
          deepseekApiKey,
          false,
        );
        console.log("[AIClient] âœ… Step 3 æˆåŠŸ: ç›´é€£ DeepSeek API");
        return result;
      } catch (error: unknown) {
        const err = error instanceof Error ? error : new Error(String(error));
        errors.push(`ç›´é€£ DeepSeek: ${err.message}`);
        console.log(`[AIClient] âš ï¸ Step 3 å¤±æ•—: ${err.message}`);
      }
    }

    // === Step 4: Gateway OpenAI ===
    const openaiApiKey = process.env.OPENAI_API_KEY;
    if (isGatewayEnabled() && openaiApiKey) {
      try {
        console.log("[AIClient] ğŸ”„ Step 4: Gateway OpenAI");
        const result = await this.callOpenAIAPI(params, openaiApiKey, true);
        console.log("[AIClient] âœ… Step 4 æˆåŠŸ: Gateway OpenAI");
        return result;
      } catch (error: unknown) {
        const err = error instanceof Error ? error : new Error(String(error));
        errors.push(`Gateway OpenAI: ${err.message}`);
        console.log(`[AIClient] âš ï¸ Step 4 å¤±æ•—: ${err.message}`);
      }
    }

    // === Step 5: ç›´é€£ OpenAI ===
    if (openaiApiKey) {
      try {
        console.log("[AIClient] ğŸ”„ Step 5: ç›´é€£ OpenAI");
        const result = await this.callOpenAIAPI(params, openaiApiKey, false);
        console.log("[AIClient] âœ… Step 5 æˆåŠŸ: ç›´é€£ OpenAI");
        return result;
      } catch (error: unknown) {
        const err = error instanceof Error ? error : new Error(String(error));
        errors.push(`ç›´é€£ OpenAI: ${err.message}`);
        console.log(`[AIClient] âš ï¸ Step 5 å¤±æ•—: ${err.message}`);
      }
    }

    // æ‰€æœ‰ fallback éƒ½å¤±æ•—
    throw new Error(
      `æ‰€æœ‰ AI Provider éƒ½å¤±æ•—äº†:\n${errors.map((e, i) => `  ${i + 1}. ${e}`).join("\n")}`,
    );
  }

  private async callDeepSeekAPIInternal(
    params: {
      model: string;
      messages: Array<{ role: string; content: string }>;
      temperature?: number;
      max_tokens?: number;
      response_format?: { type: string };
    },
    apiKey: string,
    useGateway: boolean,
  ) {
    // ç‚ºæ‰€æœ‰ DeepSeek æ¨¡å‹è¨­å®šè¼ƒé•·çš„è¶…æ™‚æ™‚é–“ï¼ˆ300 ç§’ / 5 åˆ†é˜ï¼‰
    // ç¿»è­¯ä»»å‹™éœ€è¦å¤šæ¬¡ API å‘¼å«ï¼Œæ¯æ¬¡å‘¼å«å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“
    const timeoutMs = 300000;

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

    try {
      let endpoint: string;
      let headers: Record<string, string>;

      if (useGateway) {
        // Gateway æ¨¡å¼: .../deepseek/chat/completionsï¼ˆä¸éœ€è¦ /v1ï¼‰
        const baseUrl = getDeepSeekBaseUrl();
        endpoint = `${baseUrl}/chat/completions`;
        headers = buildDeepSeekHeaders(apiKey);
      } else {
        // ç›´é€£æ¨¡å¼: https://api.deepseek.com/v1/chat/completions
        endpoint = "https://api.deepseek.com/v1/chat/completions";
        headers = {
          "Content-Type": "application/json",
          Authorization: `Bearer ${apiKey}`,
        };
      }

      console.log(
        `[AIClient] DeepSeek API (gateway: ${useGateway}, url: ${endpoint}, max_tokens: ${params.max_tokens})`,
      );

      const response = await fetch(endpoint, {
        method: "POST",
        headers,
        body: JSON.stringify({
          model: params.model,
          messages: params.messages,
          temperature: params.temperature ?? 0.7,
          max_tokens: params.max_tokens,
          response_format: params.response_format,
        }),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      // å®‰å…¨è§£æ JSON éŸ¿æ‡‰
      let responseText: string;
      try {
        responseText = await response.text();
      } catch (textError) {
        throw new Error(
          `DeepSeek API éŸ¿æ‡‰è®€å–å¤±æ•—: ${textError instanceof Error ? textError.message : "unknown"}`,
        );
      }

      let result: DeepSeekResponse;
      try {
        result = JSON.parse(responseText) as DeepSeekResponse;
      } catch (parseError) {
        // å®‰å…¨ï¼šä¸è¨˜éŒ„éŸ¿æ‡‰å…§å®¹ï¼Œåªè¨˜éŒ„éŒ¯èª¤ç‹€æ…‹
        console.error(
          `[AIClient] JSON è§£æå¤±æ•— (éŸ¿æ‡‰é•·åº¦: ${responseText.length})`,
        );
        throw new Error(
          `DeepSeek API éŸ¿æ‡‰ä¸æ˜¯æœ‰æ•ˆ JSON: ${parseError instanceof Error ? parseError.message : "parse error"}`,
        );
      }

      if (!response.ok) {
        // å®‰å…¨ï¼šåªè¨˜éŒ„éŒ¯èª¤ç¢¼ï¼Œä¸è¨˜éŒ„å®Œæ•´éŸ¿æ‡‰
        const errorObj = result as unknown as { error?: { code?: string } };
        const errorCode = errorObj.error?.code || response.status;
        throw new Error(`DeepSeek API error: ${errorCode}`);
      }

      // æª¢æŸ¥ finish_reason
      const finishReason = result.choices?.[0]?.finish_reason;
      if (finishReason === "length") {
        console.warn(
          `[AIClient] âš ï¸ è¼¸å‡ºè¢«æˆªæ–· (finish_reason=length)ï¼Œmax_tokens=${params.max_tokens}ï¼Œè€ƒæ…®å¢åŠ  token é™åˆ¶`,
        );
      }

      if (!useGateway) {
        console.log("[AIClient] âœ… ç›´é€£ DeepSeek API æˆåŠŸ");
      }

      console.log(
        `[AIClient] âœ… DeepSeek å®Œæˆ (finish_reason=${finishReason}, tokens=${result.usage?.total_tokens || "unknown"})`,
      );

      return result;
    } catch (error: unknown) {
      clearTimeout(timeoutId);
      if (error instanceof Error && error.name === "AbortError") {
        throw new Error(`DeepSeek API timeout after ${timeoutMs / 1000}s`);
      }
      throw error;
    }
  }

  /**
   * OpenAI API å‘¼å«ï¼ˆç”¨æ–¼ Fallbackï¼‰
   */
  private async callOpenAIAPI(
    params: {
      model: string;
      messages: Array<{ role: string; content: string }>;
      temperature?: number;
      max_tokens?: number;
      response_format?: { type: string };
    },
    apiKey: string,
    useGateway: boolean,
  ): Promise<DeepSeekResponse> {
    const timeoutMs = 120000; // 2 åˆ†é˜
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

    // æ¨¡å‹æ˜ å°„ï¼šdeepseek-reasoner â†’ gpt-5, deepseek-chat â†’ gpt-5-mini
    const openaiModel = params.model.includes("reasoner")
      ? "gpt-5"
      : "gpt-5-mini";

    try {
      const baseUrl = useGateway
        ? getOpenAIBaseUrl()
        : "https://api.openai.com";

      const headers = useGateway
        ? buildOpenAIHeaders(apiKey)
        : {
            "Content-Type": "application/json",
            Authorization: `Bearer ${apiKey}`,
          };

      console.log(
        `[AIClient] OpenAI API (gateway: ${useGateway}, model: ${openaiModel})`,
      );

      const response = await fetch(`${baseUrl}/v1/chat/completions`, {
        method: "POST",
        headers,
        body: JSON.stringify({
          model: openaiModel,
          messages: params.messages,
          temperature: params.temperature ?? 0.7,
          max_tokens: params.max_tokens ?? 8192,
          response_format: params.response_format,
        }),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const error = await response.json();
        throw new Error(`OpenAI API error: ${JSON.stringify(error)}`);
      }

      const data = await response.json();
      return {
        choices: data.choices.map(
          (c: { message: { role: string; content: string } }) => ({
            message: { role: c.message.role, content: c.message.content },
          }),
        ),
        usage: data.usage,
        model: data.model,
      } as DeepSeekResponse;
    } catch (error: unknown) {
      clearTimeout(timeoutId);
      if (error instanceof Error && error.name === "AbortError") {
        throw new Error(`OpenAI API timeout after ${timeoutMs / 1000}s`);
      }
      throw error;
    }
  }

  async complete(
    prompt: string | AIMessage[],
    options: AICompletionOptions,
  ): Promise<AICompletionResponse> {
    const messages = this.formatMessages(prompt);
    const maxRetries = 3;
    let lastError: Error | null = null;
    let currentModel = options.model;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        const modelSupportsJson = supportsJsonMode(currentModel);
        const wantsJsonFormat =
          options.format === "json" ||
          options.responseFormat?.type === "json_object";

        const responseFormat =
          wantsJsonFormat && modelSupportsJson
            ? { type: "json_object" }
            : undefined;

        if (
          wantsJsonFormat &&
          !modelSupportsJson &&
          process.env.NODE_ENV === "development"
        ) {
          console.debug(
            `[AIClient] Model ${currentModel} does not support JSON mode, using parser fallback`,
          );
        }

        let deepseekModel = "deepseek-chat";
        let maxTokensLimit = 8192;

        if (currentModel.includes("reasoner")) {
          deepseekModel = "deepseek-reasoner";
          maxTokensLimit = 64000;
        } else {
          deepseekModel = "deepseek-chat";
          maxTokensLimit = 8192;
        }

        const maxTokens = Math.min(options.maxTokens ?? 8000, maxTokensLimit);

        let response = await this.callDeepSeekAPI({
          model: deepseekModel,
          messages,
          temperature: options.temperature ?? 0.7,
          max_tokens: maxTokens,
          response_format: responseFormat,
        });

        // æˆªæ–·é‡è©¦é‚è¼¯ï¼šå¦‚æœ finish_reason=lengthï¼Œè‡ªå‹•å¢åŠ  token é‡è©¦ä¸€æ¬¡
        const finishReason = response.choices?.[0]?.finish_reason;
        if (
          finishReason === "length" &&
          attempt === 1 &&
          maxTokens < maxTokensLimit
        ) {
          const increasedTokens = Math.min(
            Math.floor(maxTokens * 1.5),
            maxTokensLimit,
          );
          console.warn(
            `[AIClient] âš ï¸ è¼¸å‡ºè¢«æˆªæ–·ï¼Œè‡ªå‹•é‡è©¦ (${maxTokens} â†’ ${increasedTokens} tokens)`,
          );

          response = await this.callDeepSeekAPI({
            model: deepseekModel,
            messages,
            temperature: options.temperature ?? 0.7,
            max_tokens: increasedTokens,
            response_format: responseFormat,
          });
        }

        if (currentModel !== options.model) {
          console.log(
            `[AIClient] âœ… Fallback æˆåŠŸä½¿ç”¨: ${currentModel} (åŸ: ${options.model})`,
          );
        }

        const message = response.choices[0].message;

        const content =
          message.content ||
          message.reasoning_content ||
          message.reasoning ||
          message.thinking ||
          "";

        console.log("[AIClient] DeepSeek response extraction:", {
          hasContent: !!message.content,
          hasReasoningContent: !!message.reasoning_content,
          hasReasoning: !!message.reasoning,
          hasThinking: !!message.thinking,
          contentLength: content?.length || 0,
        });

        const totalTokens = response.usage?.total_tokens || 0;

        return {
          content,
          usage: {
            promptTokens: response.usage?.prompt_tokens || 0,
            completionTokens: response.usage?.completion_tokens || 0,
            totalTokens,
          },
          model: response.model,
        };
      } catch (error: unknown) {
        const err = error instanceof Error ? error : new Error(String(error));
        lastError = err;
        const isRateLimit =
          err.message?.includes("rate-limited") ||
          err.message?.includes("429") ||
          err.message?.includes("Rate limit");

        const isTimeout =
          err.message?.includes("timeout") ||
          err.message?.includes("terminated") ||
          err.name === "AbortError";

        // å¦‚æœ deepseek-reasoner è¶…æ™‚ï¼Œè‡ªå‹•åˆ‡æ›åˆ° deepseek-chat
        if (isTimeout && currentModel.includes("reasoner") && attempt === 1) {
          console.log(
            `[AIClient] âš ï¸ ${currentModel} timeout, switching to deepseek-chat`,
          );
          currentModel = "deepseek-chat";
          continue;
        }

        // å¦‚æœ timeout ä¸”é‚„æœ‰é‡è©¦æ¬¡æ•¸ï¼Œç­‰å¾…å¾Œé‡è©¦
        if (isTimeout && attempt < maxRetries) {
          const delay = 3000;
          console.log(
            `[AIClient] âš ï¸ Timeout, retrying in ${delay}ms (attempt ${attempt}/${maxRetries})`,
          );
          await new Promise((resolve) => setTimeout(resolve, delay));
          continue;
        }

        if (isRateLimit && attempt < maxRetries) {
          const delays = [5000, 10000, 20000];
          const delay = delays[attempt - 1] || 20000;
          console.log(
            `[AIClient] Rate limit hit, retrying in ${delay}ms (attempt ${attempt}/${maxRetries})`,
          );
          await new Promise((resolve) => setTimeout(resolve, delay));
          continue;
        }

        throw new Error(`AI completion failed: ${err.message}`);
      }
    }

    throw new Error(
      `AI completion failed after ${maxRetries} attempts: ${lastError?.message}`,
    );
  }

  private formatMessages(prompt: string | AIMessage[]): AIMessage[] {
    if (typeof prompt === "string") {
      return [{ role: "user", content: prompt }];
    }
    return prompt;
  }

  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  private async tryGenerateWithRetries(
    prompt: string,
    options: {
      model: string;
      quality?: "low" | "medium" | "high" | "auto";
      size?: string;
    },
    maxRetries: number,
  ): Promise<{
    success: boolean;
    data?: { url: string; revisedPrompt?: string };
    error?: Error;
  }> {
    let lastError: Error | null = null;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(
          `[AIClient] ğŸ¨ Image attempt ${attempt}/${maxRetries} with ${options.model}`,
        );
        const result = await this.generateImageWithModel(prompt, options);
        return { success: true, data: result };
      } catch (error) {
        lastError = error as Error;
        console.warn(
          `[AIClient] âš ï¸ Attempt ${attempt} failed: ${lastError.message}`,
        );

        if (attempt < maxRetries) {
          const delay = 2000 * attempt;
          console.log(`[AIClient] â³ Waiting ${delay}ms before retry...`);
          await this.sleep(delay);
        }
      }
    }

    return { success: false, error: lastError! };
  }

  /**
   * åœ–ç‰‡ç”Ÿæˆï¼ˆçµ±ä¸€ä½¿ç”¨ fal.ai qwen-imageï¼‰
   * å„ªé»ï¼šé€Ÿåº¦å¿«ï¼ˆç´„ 3 ç§’ï¼‰ã€æˆæœ¬ä½ã€ç›´æ¥è¿”å› URL
   */
  async generateImage(
    prompt: string,
    options: {
      model: string;
      quality?: "low" | "medium" | "high" | "auto";
      size?: string;
    },
  ): Promise<{ url: string; revisedPrompt?: string }> {
    const MAX_RETRIES = 3;

    // çµ±ä¸€ä½¿ç”¨ fal-ai Seedream v4ï¼ˆæ”¯æ´æ–‡å­—æ¸²æŸ“ï¼‰
    const result = await this.tryGenerateWithRetries(
      prompt,
      { ...options, model: "fal-ai/bytedance/seedream/v4/text-to-image" },
      MAX_RETRIES,
    );

    if (result.success && result.data) {
      return result.data;
    }

    throw result.error || new Error("Image generation failed");
  }

  private async generateImageWithModel(
    prompt: string,
    options: {
      model: string;
      quality?: "low" | "medium" | "high" | "auto";
      size?: string;
    },
  ): Promise<{ url: string; revisedPrompt?: string }> {
    // å„ªå…ˆä½¿ç”¨ fal.aiï¼ˆSeedream v4 æˆ– qwen-imageï¼‰
    if (
      options.model.includes("fal-ai") ||
      options.model.includes("seedream") ||
      options.model.includes("qwen-image")
    ) {
      return await this.callFalImageAPI(prompt, options);
    }

    // å…¶ä»–æ¨¡å‹çš„ fallbackï¼ˆä¿ç•™ä½†ä¸ä¸»å‹•ä½¿ç”¨ï¼‰
    if (
      options.model.includes("gemini-imagen") ||
      options.model.includes("imagen-3") ||
      options.model.includes("gemini-2.5-flash-image") ||
      options.model.includes("gemini-3-pro-image-preview")
    ) {
      return await this.callGeminiImagenAPI(prompt, options);
    }

    throw new Error(`Unsupported image model: ${options.model}`);
  }

  /**
   * å‘¼å« fal.ai APIï¼ˆæ”¯æ´ Seedream v4 å’Œ qwen-imageï¼‰
   * Seedream v4: æ”¯æ´æ–‡å­—æ¸²æŸ“ã€é«˜å“è³ª
   */
  private async callFalImageAPI(
    prompt: string,
    options: {
      model: string;
      quality?: "low" | "medium" | "high" | "auto";
      size?: string;
    },
  ): Promise<{ url: string; revisedPrompt?: string }> {
    // åˆ¤æ–·ä½¿ç”¨çš„æ¨¡å‹
    const isSeedream = options.model.includes("seedream");
    const modelPath = isSeedream
      ? "fal-ai/bytedance/seedream/v4/text-to-image"
      : "fal-ai/qwen-image";

    // Seedream v4 ä½¿ç”¨ auto_2K æ ¼å¼ï¼Œqwen-image ä½¿ç”¨èˆŠæ ¼å¼
    let imageSize: string;
    if (isSeedream) {
      // Seedream v4 æ”¯æ´ auto_2K, auto_4K æˆ–è‡ªè¨‚å°ºå¯¸
      imageSize = "auto_2K";
    } else {
      // qwen-image ä½¿ç”¨èˆŠæ ¼å¼
      const imageSizeMap: Record<string, string> = {
        "1024x1024": "square_hd",
        "1792x1024": "landscape_16_9",
        "1024x1792": "portrait_16_9",
        "1280x720": "landscape_16_9",
        "720x1280": "portrait_16_9",
      };
      imageSize = imageSizeMap[options.size || "1024x1024"] || "landscape_16_9";
    }

    const falUrl = buildFalApiUrl(modelPath);
    const falHeaders = buildFalHeaders();

    console.log(
      `[AIClient] ğŸ¨ Calling fal.ai ${isSeedream ? "Seedream v4" : "qwen-image"} (size: ${imageSize}, gateway: ${isGatewayEnabled()})`,
    );

    // Seedream v4 ä¸éœ€è¦ num_inference_steps å’Œ guidance_scale
    const requestBody: Record<string, unknown> = {
      prompt,
      image_size: imageSize,
      num_images: 1,
      // é—œé–‰ safety checkerï¼šä¸­æ–‡å•†æ¥­å…§å®¹ç¶“å¸¸è¢«èª¤åˆ¤ç‚º content_policy_violation
      enable_safety_checker: false,
    };

    // åªæœ‰ qwen-image éœ€è¦é€™äº›åƒæ•¸
    if (!isSeedream) {
      const stepsMap: Record<string, number> = {
        low: 20,
        medium: 28,
        high: 35,
        auto: 28,
      };
      requestBody.num_inference_steps = stepsMap[options.quality || "medium"];
      requestBody.guidance_scale = 3.5;
      requestBody.output_format = "png";
    }

    const response = await fetch(falUrl, {
      method: "POST",
      headers: falHeaders,
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`fal.ai API error (${response.status}): ${errorText}`);
    }

    const data = await response.json();

    if (!data.images || !data.images[0]?.url) {
      throw new Error("Invalid fal.ai response: no image URL");
    }

    console.log(
      `[AIClient] âœ… fal.ai ${isSeedream ? "Seedream v4" : "qwen-image"} generated (${data.timings?.inference?.toFixed(2) || "N/A"}s)`,
    );

    return {
      url: data.images[0].url,
      revisedPrompt: data.prompt || prompt,
    };
  }

  private mapGeminiImageModel(model: string): string {
    const modelMap: Record<string, string> = {
      "gemini-3-pro-image-preview": "gemini-3-pro-image-preview",
      "gemini-imagen-flash": "gemini-2.5-flash-image",
      "gemini-imagen": "gemini-2.5-flash-image",
      "imagen-3": "gemini-2.5-flash-image",
      "gemini-2.5-flash-image": "gemini-2.5-flash-image",
    };
    return modelMap[model] || model;
  }

  private async callGeminiImagenAPI(
    prompt: string,
    options: {
      model: string;
      quality?: "low" | "medium" | "high" | "auto";
      size?: string;
    },
  ): Promise<{ url: string; revisedPrompt?: string }> {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      throw new Error("GEMINI_API_KEY is not set");
    }

    const modelName = this.mapGeminiImageModel(options.model);

    const [width, height] = (options.size || "1024x1024")
      .split("x")
      .map(Number);
    const aspectRatio =
      width === height ? "1:1" : width > height ? "16:9" : "9:16";

    console.log(
      `[AIClient] ğŸ¨ Calling Gemini Image API (model: ${modelName}, requested: ${options.model}, aspect: ${aspectRatio}, gateway: ${isGatewayEnabled()})`,
    );

    // Gateway æ¨¡å¼ä½¿ç”¨ Gateway URLï¼Œç›´é€£æ¨¡å¼ä½¿ç”¨å®˜æ–¹ URLï¼ˆå¸¶ key åƒæ•¸ï¼‰
    const geminiUrl = isGatewayEnabled()
      ? buildGeminiApiUrl(modelName, "generateContent")
      : `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`;

    const geminiHeaders = buildGeminiHeaders(apiKey);

    const response = await fetch(geminiUrl, {
      method: "POST",
      headers: geminiHeaders,
      body: JSON.stringify({
        contents: [
          {
            parts: [{ text: prompt }],
          },
        ],
        generationConfig: {
          responseModalities: ["IMAGE"],
          imageConfig: {
            aspectRatio,
          },
        },
      }),
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Gemini Image API error: ${JSON.stringify(error)}`);
    }

    const data = await response.json();

    const finishReason = data.candidates?.[0]?.finishReason;
    if (finishReason === "NO_IMAGE") {
      throw new Error(`[NO_IMAGE] Gemini refused to generate image for prompt`);
    }

    const inlineData = data.candidates?.[0]?.content?.parts?.[0]?.inlineData;
    if (!inlineData?.data) {
      throw new Error(
        "Invalid Gemini Image response structure: " + JSON.stringify(data),
      );
    }

    const base64Data = inlineData.data;
    const mimeType = inlineData.mimeType || "image/png";

    const dataUrl = `data:${mimeType};base64,${base64Data}`;
    console.log(
      "[AIClient] âœ… Gemini Image generated successfully (base64 length:",
      base64Data.length,
      ")",
    );

    return {
      url: dataUrl,
      revisedPrompt: prompt,
    };
  }

  private async callGeminiImageAPI(
    prompt: string,
    options: {
      model: string;
      quality?: "low" | "medium" | "high" | "auto";
      size?: string;
    },
  ): Promise<{ url: string; revisedPrompt?: string }> {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      throw new Error("GEMINI_API_KEY is not set");
    }

    const [width, height] = (options.size || "1024x1024")
      .split("x")
      .map(Number);
    let aspectRatio = "1:1";
    if (width > height) {
      aspectRatio = width / height >= 1.7 ? "16:9" : "4:3";
    } else if (height > width) {
      aspectRatio = height / width >= 1.7 ? "9:16" : "3:4";
    }

    const modelName = options.model.includes("gemini-3-pro")
      ? "gemini-3-pro-image-preview"
      : "gemini-2.5-flash-image";

    console.log(
      `[AIClient] ğŸ¨ Calling Gemini Image API (model: ${modelName}, aspect: ${aspectRatio}, gateway: ${isGatewayEnabled()})`,
    );

    // Gateway æ¨¡å¼ä½¿ç”¨ Gateway URLï¼Œç›´é€£æ¨¡å¼ä½¿ç”¨å®˜æ–¹ URLï¼ˆå¸¶ key åƒæ•¸ï¼‰
    const geminiImageUrl = isGatewayEnabled()
      ? buildGeminiApiUrl(modelName, "generateContent")
      : `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`;

    const geminiImageHeaders = buildGeminiHeaders(apiKey);

    const response = await fetch(geminiImageUrl, {
      method: "POST",
      headers: geminiImageHeaders,
      body: JSON.stringify({
        contents: [
          {
            parts: [{ text: prompt }],
          },
        ],
        generationConfig: {
          responseModalities: ["TEXT", "IMAGE"],
        },
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(
        `Gemini Image API error (${response.status}): ${errorText}`,
      );
    }

    const data = await response.json();

    const finishReason = data.candidates?.[0]?.finishReason;
    if (finishReason === "NO_IMAGE") {
      throw new Error(`[NO_IMAGE] Gemini refused to generate image for prompt`);
    }

    if (!data.candidates || !data.candidates[0]?.content?.parts) {
      throw new Error("Invalid Gemini Image API response structure");
    }

    const parts = data.candidates[0].content.parts;
    let imageData: string | null = null;
    let mimeType = "image/png";
    let description = prompt;

    for (const part of parts) {
      if (part.inlineData) {
        imageData = part.inlineData.data;
        mimeType = part.inlineData.mimeType || "image/png";
      } else if (part.text) {
        description = part.text;
      }
    }

    if (!imageData) {
      throw new Error("No image data in Gemini response");
    }

    const dataUrl = `data:${mimeType};base64,${imageData}`;

    console.log(
      "[AIClient] âœ… Gemini Image generated successfully (base64 length:",
      imageData.length,
      ")",
    );

    return {
      url: dataUrl,
      revisedPrompt: description,
    };
  }
}
