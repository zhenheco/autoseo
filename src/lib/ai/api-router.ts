/**
 * AI API Router
 * æ ¹æ“šæ¨¡å‹çš„ api_provider é¸æ“‡æ­£ç¢ºçš„ API å®¢æˆ¶ç«¯
 */

import { getDeepSeekClient } from '@/lib/deepseek/client';
import { callOpenRouter } from '@/lib/openrouter';
import { getOpenAIImageClient } from '@/lib/openai/image-client';
import { getOpenAITextClient } from '@/lib/openai/text-client';
import { getPerplexityClient } from '@/lib/perplexity/client';
import type {
  APIProvider,
  ProcessingTier,
  TokenUsage,
  UnifiedAPIResponse,
} from '@/types/ai-models';
import type { AIMessage } from '@/types/agents';

export interface APIRouterConfig {
  deepseekApiKey?: string;
  openrouterApiKey?: string;
  openaiApiKey?: string;
  perplexityApiKey?: string;
  maxRetries?: number;
  timeout?: number;
  enableFallback?: boolean;
}

export interface TextCompletionOptions {
  model: string;
  apiProvider: APIProvider;
  prompt: string | AIMessage[];
  temperature?: number;
  maxTokens?: number;
  responseFormat?: 'text' | 'json';
}

export interface ImageGenerationOptions {
  model: string;
  apiProvider: APIProvider;
  prompt: string;
  size?: string;
  quality?: 'standard' | 'hd';
  count?: number;
}

/**
 * AI API Router - çµ±ä¸€çš„ API è·¯ç”±å™¨
 */
export class APIRouter {
  private config: APIRouterConfig;
  private fallbackChains: Record<string, string[]>;

  constructor(config: APIRouterConfig = {}) {
    this.config = config;

    // é è¨­ Fallback éˆ
    this.fallbackChains = {
      complex: [
        'deepseek-reasoner',
        'openai/gpt-5',
        'openai/gpt-4o',
        'google/gemini-2.5-pro',
        'google/gemini-2.5-flash',
        'anthropic/claude-sonnet-4.5',
      ],
      simple: [
        'deepseek-chat',
        'openai/gpt-5-mini',
        'openai/gpt-4o-mini',
        'openai/gpt-4o',
        'anthropic/claude-sonnet-4.5',
      ],
    };
  }

  /**
   * æ–‡å­—å®Œæˆï¼ˆæ ¹æ“š api_provider è·¯ç”±ï¼‰
   */
  async complete(options: TextCompletionOptions): Promise<UnifiedAPIResponse> {
    const messages = typeof options.prompt === 'string'
      ? [{ role: 'user' as const, content: options.prompt }]
      : options.prompt;

    let lastError: Error | null = null;
    const enableFallback = this.config.enableFallback !== false;
    const maxAttempts = enableFallback ? 3 : 1;

    // å–å¾— Fallback éˆ
    const tier = this.detectProcessingTier(options.model);
    const fallbackChain = [...this.fallbackChains[tier] || []];
    let currentModel = options.model;
    let currentProvider = options.apiProvider;

    for (let attempt = 1; attempt <= maxAttempts; attempt++) {
      try {
        const result = await this.routeTextRequest(
          currentModel,
          currentProvider,
          messages,
          options.temperature,
          options.maxTokens,
          options.responseFormat
        );

        // å¦‚æœä½¿ç”¨äº† fallbackï¼Œè¨˜éŒ„æ—¥èªŒ
        if (attempt > 1) {
          console.log(`[APIRouter] âœ… Fallback æˆåŠŸ: ${currentModel} (åŸ: ${options.model}, å˜—è©¦: ${attempt})`);
        }

        return result;
      } catch (error: unknown) {
        lastError = error as Error;
        const errorMessage = (error as Error).message || '';

        // Rate limit æˆ–æœå‹™å™¨éŒ¯èª¤ï¼Œå˜—è©¦ fallback
        const isRateLimitOrServerError =
          errorMessage.includes('429') ||
          errorMessage.includes('rate-limited') ||
          errorMessage.includes('Rate limit') ||
          errorMessage.includes('500') ||
          errorMessage.includes('502') ||
          errorMessage.includes('503');

        if (isRateLimitOrServerError && enableFallback && attempt < maxAttempts) {
          const nextModel = this.getNextFallbackModel(currentModel, fallbackChain);

          if (nextModel) {
            console.log(`[APIRouter] âš ï¸ ${currentModel} å¤±æ•— (${errorMessage})`);
            console.log(`[APIRouter] ğŸ”„ åˆ‡æ›åˆ° Fallback: ${nextModel}`);

            currentModel = nextModel;
            currentProvider = this.detectAPIProvider(nextModel);
            continue;
          }
        }

        // å…¶ä»–éŒ¯èª¤æˆ–ç„¡ fallback å¯ç”¨
        if (attempt < maxAttempts) {
          const delay = Math.min(1000 * Math.pow(2, attempt), 10000);
          console.log(`[APIRouter] â³ é‡è©¦ä¸­... (${delay}ms å¾Œç¬¬ ${attempt + 1} æ¬¡)`);
          await this.sleep(delay);
          continue;
        }

        throw error;
      }
    }

    throw new Error(
      `API è«‹æ±‚å¤±æ•—ï¼ˆå·²é‡è©¦ ${maxAttempts} æ¬¡ï¼‰: ${lastError?.message || 'Unknown error'}`
    );
  }

  /**
   * åœ–ç‰‡ç”Ÿæˆ
   */
  async generateImage(options: ImageGenerationOptions): Promise<{
    url: string;
    revisedPrompt?: string;
  }> {
    if (options.apiProvider !== 'openai') {
      throw new Error(`åœ–ç‰‡ç”Ÿæˆç›®å‰åªæ”¯æ´ OpenAI APIï¼Œæ”¶åˆ°: ${options.apiProvider}`);
    }

    const client = getOpenAIImageClient({
      apiKey: this.config.openaiApiKey,
    });

    return client.generateImage({
      model: options.model,
      prompt: options.prompt,
      size: options.size,
      quality: options.quality,
    });
  }

  /**
   * æ‰¹é‡åœ–ç‰‡ç”Ÿæˆ
   */
  async generateImages(options: ImageGenerationOptions & { count: number }): Promise<Array<{
    url: string;
    revisedPrompt?: string;
  }>> {
    if (options.apiProvider !== 'openai') {
      throw new Error(`åœ–ç‰‡ç”Ÿæˆç›®å‰åªæ”¯æ´ OpenAI APIï¼Œæ”¶åˆ°: ${options.apiProvider}`);
    }

    const client = getOpenAIImageClient({
      apiKey: this.config.openaiApiKey,
    });

    return client.generateMultiple({
      model: options.model,
      prompt: options.prompt,
      count: options.count,
      size: options.size,
      quality: options.quality,
    });
  }

  /**
   * è·¯ç”±æ–‡å­—è«‹æ±‚åˆ°æ­£ç¢ºçš„ API
   */
  private async routeTextRequest(
    model: string,
    apiProvider: APIProvider,
    messages: AIMessage[],
    temperature?: number,
    maxTokens?: number,
    responseFormat?: 'text' | 'json'
  ): Promise<UnifiedAPIResponse> {
    switch (apiProvider) {
      case 'deepseek':
        return this.callDeepSeekAPI(model, messages, temperature, maxTokens, responseFormat);

      case 'openai':
        return this.callOpenAIAPI(model, messages, temperature, maxTokens, responseFormat);

      case 'openrouter':
        return this.callOpenRouterAPI(model, messages, temperature, maxTokens, responseFormat);

      case 'perplexity':
        return this.callPerplexityAPI(model, messages, temperature, maxTokens);

      default:
        throw new Error(`ä¸æ”¯æ´çš„ API Provider: ${apiProvider}`);
    }
  }

  /**
   * å‘¼å« DeepSeek å®˜æ–¹ API
   */
  private async callDeepSeekAPI(
    model: string,
    messages: AIMessage[],
    temperature?: number,
    maxTokens?: number,
    responseFormat?: 'text' | 'json'
  ): Promise<UnifiedAPIResponse> {
    const client = getDeepSeekClient({
      apiKey: this.config.deepseekApiKey,
      maxRetries: this.config.maxRetries,
      timeout: this.config.timeout,
    });

    // DeepSeek åªæ¥å— deepseek-reasoner æˆ– deepseek-chat
    const deepseekModel = model.includes('reasoner') ? 'deepseek-reasoner' : 'deepseek-chat';

    const result = await client.complete({
      model: deepseekModel as 'deepseek-reasoner' | 'deepseek-chat',
      prompt: messages,
      temperature,
      max_tokens: maxTokens,
      responseFormat,
    });

    return {
      content: result.content,
      usage: {
        input_tokens: result.usage.prompt_tokens,
        output_tokens: result.usage.completion_tokens,
        total_tokens: result.usage.total_tokens,
        billing_input_tokens: result.usage.prompt_tokens * 2,
        billing_output_tokens: result.usage.completion_tokens * 2,
        total_billing_tokens: result.usage.total_tokens * 2,
      },
      model: result.model,
      api_provider: 'deepseek',
    };
  }

  /**
   * å‘¼å« OpenAI å®˜æ–¹ API
   */
  private async callOpenAIAPI(
    model: string,
    messages: AIMessage[],
    temperature?: number,
    maxTokens?: number,
    responseFormat?: 'text' | 'json'
  ): Promise<UnifiedAPIResponse> {
    const client = getOpenAITextClient({
      apiKey: this.config.openaiApiKey,
      maxRetries: this.config.maxRetries,
      timeout: this.config.timeout,
    });

    const result = await client.complete({
      model: model.replace('openai/', ''),
      messages,
      temperature,
      max_tokens: maxTokens,
      response_format: responseFormat === 'json' ? { type: 'json_object' } : undefined,
    });

    return {
      content: result.content,
      usage: {
        input_tokens: result.usage.prompt_tokens,
        output_tokens: result.usage.completion_tokens,
        total_tokens: result.usage.total_tokens,
        billing_input_tokens: result.usage.prompt_tokens,
        billing_output_tokens: result.usage.completion_tokens,
        total_billing_tokens: result.usage.total_tokens,
      },
      model: result.model,
      api_provider: 'openai',
    };
  }

  /**
   * å‘¼å« OpenRouter API
   */
  private async callOpenRouterAPI(
    model: string,
    messages: AIMessage[],
    temperature?: number,
    maxTokens?: number,
    responseFormat?: 'text' | 'json'
  ): Promise<UnifiedAPIResponse> {
    const response = await callOpenRouter({
      model,
      messages,
      temperature: temperature ?? 0.7,
      max_tokens: maxTokens ?? 2000,
      response_format: responseFormat === 'json' ? { type: 'json_object' } : undefined,
    });

    return {
      content: response.choices[0].message.content || '',
      usage: {
        input_tokens: response.usage?.prompt_tokens || 0,
        output_tokens: response.usage?.completion_tokens || 0,
        total_tokens: response.usage?.total_tokens || 0,
        billing_input_tokens: (response.usage?.prompt_tokens || 0) * 2,
        billing_output_tokens: (response.usage?.completion_tokens || 0) * 2,
        total_billing_tokens: (response.usage?.total_tokens || 0) * 2,
      },
      model: response.model,
      api_provider: 'openrouter',
    };
  }

  /**
   * å‘¼å« Perplexity API
   */
  private async callPerplexityAPI(
    model: string,
    messages: AIMessage[],
    temperature?: number,
    maxTokens?: number
  ): Promise<UnifiedAPIResponse> {
    const client = getPerplexityClient();

    const query = messages.map(m => m.content).join('\n');

    const result = await client.search(query, {
      model,
      temperature,
      max_tokens: maxTokens,
    });

    return {
      content: result.content,
      usage: {
        input_tokens: 0,
        output_tokens: 0,
        total_tokens: 0,
        billing_input_tokens: 0,
        billing_output_tokens: 0,
        total_billing_tokens: 0,
      },
      model: model,
      api_provider: 'perplexity',
    };
  }

  /**
   * åµæ¸¬æ¨¡å‹çš„è™•ç†éšæ®µ
   */
  private detectProcessingTier(model: string): string {
    if (model.includes('reasoner') || model.includes('gpt-5') || model.includes('gemini-2.5-pro')) {
      return 'complex';
    }
    return 'simple';
  }

  /**
   * åµæ¸¬æ¨¡å‹çš„ API Provider
   */
  private detectAPIProvider(model: string): APIProvider {
    if (model.startsWith('deepseek') || model === 'deepseek-reasoner' || model === 'deepseek-chat') {
      return 'deepseek';
    }
    if (model.startsWith('openai/') || model.includes('gpt-') || model.includes('dall-e') || model.includes('gpt-image')) {
      return 'openai';
    }
    if (model.startsWith('google/') || model.startsWith('anthropic/')) {
      return 'openrouter';
    }
    if (model.includes('sonar')) {
      return 'perplexity';
    }
    return 'openrouter'; // é è¨­ä½¿ç”¨ OpenRouter
  }

  /**
   * å–å¾—ä¸‹ä¸€å€‹ Fallback æ¨¡å‹
   */
  private getNextFallbackModel(currentModel: string, chain: string[]): string | null {
    const currentIndex = chain.indexOf(currentModel);
    if (currentIndex === -1 || currentIndex >= chain.length - 1) {
      return null;
    }
    return chain[currentIndex + 1];
  }

  /**
   * Sleep å·¥å…·å‡½æ•¸
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

/**
 * å…¨åŸŸ API Router å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
 */
let globalRouter: APIRouter | null = null;

export function getAPIRouter(config?: APIRouterConfig): APIRouter {
  if (!globalRouter) {
    globalRouter = new APIRouter({
      deepseekApiKey: process.env.DEEPSEEK_API_KEY,
      openrouterApiKey: process.env.OPENROUTER_API_KEY,
      openaiApiKey: process.env.OPENAI_API_KEY,
      perplexityApiKey: process.env.PERPLEXITY_API_KEY,
      maxRetries: 3,
      timeout: 120000,
      enableFallback: true,
      ...config,
    });
  }
  return globalRouter;
}

/**
 * é‡ç½®å…¨åŸŸ Routerï¼ˆä¸»è¦ç”¨æ–¼æ¸¬è©¦ï¼‰
 */
export function resetAPIRouter(): void {
  globalRouter = null;
}

// é è¨­å°å‡º
export default APIRouter;
